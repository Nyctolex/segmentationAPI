{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from models.load_model import load_torch_model\n",
    "from models.model_wrappers import SegmentationModelAI, ImageType, OnnxSegmentationWrapper\n",
    "from models.utils import ImageToVector, load_dummy_image, torch_to_onnx\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "from models.utils import segment_prediction_to_image, vector_to_pil\n",
    "# Loading the model\n",
    "model, preprocessor = load_torch_model()\n",
    "dummy_batch = preprocessor(load_dummy_image()).unsqueeze(0)\n",
    "\n",
    "# Converting it to an onnx model\n",
    "model = torch_to_onnx(model, dummy_batch)\n",
    "# preprocessor = torch_to_onnx(preprocessor, dummy_batch)\n",
    "# setting up the wrapper\n",
    "# model = SegmentationModelAI(model, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessor.get_inputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessor.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_wrappers import ModelWrapper, InferenceSession, Optional, Callable\n",
    "class OnnxWrapper(ModelWrapper):\n",
    "\n",
    "    def __init__(self, model: InferenceSession, preprocessor: Optional[Callable] = None):\n",
    "        super().__init__(model, preprocessor)\n",
    "        self.input_names = [x.name for x in self.model.get_inputs()]\n",
    "        self.output_names = [x.name for x in self.model.get_outputs()]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def supported_model_types(self):\n",
    "        return (InferenceSession,)\n",
    "\n",
    "    def predict_batch(self, images: np.array) -> np.array:\n",
    "        \"\"\"Prediction of a batch\n",
    "\n",
    "        Args:\n",
    "            images (np.array): batch of images in shape [N, 3, H, W]\n",
    "\n",
    "        Returns:\n",
    "            np.array: the result of the model\n",
    "        \"\"\"\n",
    "        if self.preprocessor:\n",
    "            images = self.preprocessor(images)\n",
    "        model_input = {self.model.get_inputs()[0].name: images}\n",
    "        return self.model.run(self.output_names, model_input)[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def permute_image(image: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Permute image of [H, W, 3] to [3, H, W] if needed\n",
    "        \"\"\"\n",
    "        if image.shape[0] != 3 and image.shape[-1] == 3:\n",
    "            image = np.transpose(image, (2, 1, 0))\n",
    "        return image\n",
    "    \n",
    "    def predict_single(self, image: ImageType) -> np.array:\n",
    "        \"\"\"Prediction of a single data point (unbatched)\n",
    "\n",
    "        Args:\n",
    "            image (ImageType): The input image (unbatched) of shape [3, H, W]\n",
    "\n",
    "        Returns:\n",
    "            np.array: The prediction of the model\n",
    "        \"\"\"\n",
    "        image = ImageToVector.to_numpy(image)\n",
    "        image = self.permute_image(image)\n",
    "        # Expand dimmentions\n",
    "        image = image[np.newaxis, :]\n",
    "        return self.predict_batch(image)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x12cb705f0> <class 'onnxruntime.capi.onnxruntime_inference_collection.InferenceSession'>\n"
     ]
    }
   ],
   "source": [
    "w = OnnxWrapper(torch_to_onnx(preprocessor, dummy_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: img for the following indices\n index: 1 Got: 1026 Expected: 3\n index: 2 Got: 1282 Expected: 520\n index: 3 Got: 3 Expected: 649\n Please fix either the inputs/outputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(i, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)[np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# preprocessor(i)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mOnnxWrapper.predict_batch\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     24\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor(images)\n\u001b[1;32m     25\u001b[0m model_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname: images}\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/DeepKeep/.venv/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: img for the following indices\n index: 1 Got: 1026 Expected: 3\n index: 2 Got: 1282 Expected: 520\n index: 3 Got: 3 Expected: 649\n Please fix either the inputs/outputs or the model."
     ]
    }
   ],
   "source": [
    "i = load_dummy_image()\n",
    "i = np.array(i, dtype=np.float32)[np.newaxis, :]\n",
    "# preprocessor(i)\n",
    "w.predict_batch(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1026, 1282, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
